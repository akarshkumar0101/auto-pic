{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run imports_notebook.py\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import clip\n",
    "from PIL import Image\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "from image_cppn import ImageCPPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]):\n",
    "    \"\"\"\n",
    "    img.shape should be (3, h, w) on any device, torch tensor\n",
    "    \"\"\"\n",
    "    img = img.detach().cpu()\n",
    "#     mean = torch.tensor(mean)\n",
    "#     std = torch.tensor(std)\n",
    "    img = img.permute(1, 2, 0)\n",
    "#     img = img*std+mean\n",
    "    img = img.clamp(0, 1).numpy()\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP nouns\n",
    "# Ask GPT3: Here is a list of fifty objects from nature:\n",
    "nouns = ['a rock', 'a tree', 'a flower', 'a blade of grass', 'a leaf', 'a cloud', 'the sun', 'the moon', 'the stars', 'the sky', 'a river', 'a waterfall', 'the ocean', 'a wave',\n",
    "        'a stone', 'a pebble', 'a grain of sand', 'a drop of water', 'a raindrop', 'a snowflake', 'a flake of snow', 'an ice cube', 'a piece of ice', 'a glacier', 'a piece of wood',\n",
    "         'a log', 'a stick', 'a twig', 'a branch', 'a tree trunk', 'a piece of bark', 'a nut', 'a seed', 'a fruit', 'a vegetable', 'a leaf', 'a root', 'a bulb', 'a flower',\n",
    "         'a stem', 'a vine', 'a root system', 'a mountain', 'a lake', 'a cliff', 'a cave', 'an apple', 'a banana', 'a pineapple']\n",
    "print(len(nouns))\n",
    "NUM_IMAGES_SQRD = 7\n",
    "NUM_IMAGES = 49\n",
    "\n",
    "text = clip.tokenize(['a photo of ' + x for x in nouns]).to(device)\n",
    "print(text.shape, text.dtype, text.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text).detach()\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_trans = transforms.RandomResizedCrop(224, scale=(1.0, 1.0))\n",
    "\n",
    "def display_archive(archive, nouns, updated):\n",
    "    with torch.no_grad():\n",
    "        imgs = torch.stack([cppn.generate_image((64, 64))[0] for cppn in archive])\n",
    "        imgs = resize_trans(imgs)\n",
    "        imgs_features = model.encode_image(imgs)\n",
    "\n",
    "        clip_scores = torch.cosine_similarity(imgs_features, text_features, dim=-1)\n",
    "\n",
    "        plt.figure(figsize=(NUM_IMAGES_SQRD*2, NUM_IMAGES_SQRD*2.3))\n",
    "        for i_img, img in enumerate(imgs):\n",
    "            plt.subplot(NUM_IMAGES_SQRD, NUM_IMAGES_SQRD, i_img+1)\n",
    "            plt.axis('off')\n",
    "            if updated[i_img]:\n",
    "                plt.title('{}\\n[{:.4f}]'.format(nouns[i_img], clip_scores[i_img].item()), fontweight='bold')\n",
    "            else:\n",
    "                plt.title('{}\\n*[{:.4f}]*'.format(nouns[i_img], clip_scores[i_img].item()))\n",
    "            imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Archive of CPPNs, one for each niche\n",
    "# archive = []\n",
    "# for i in range(NUM_IMAGES):\n",
    "#     cppn = ImageCPPN(n_hidden=20, n_layers=8, n_channels=3, activation=torch.relu, give_radius=True).to(device)\n",
    "#     archive.append(cppn)\n",
    "    \n",
    "# # display_archive(archive, nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_trans = transforms.Compose([\n",
    "#     transforms.RandomPerspective(distortion_scale=0.5, p=1, fill=1),\n",
    "#     transforms.RandomResizedCrop(224, scale=(0.7,0.9)),\n",
    "#     transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "# ])\n",
    "\n",
    "# params = []\n",
    "# for cppn in archive:\n",
    "#     params += list(cppn.parameters())\n",
    "# optim = torch.optim.Adam(params, lr=1e-3)\n",
    "    \n",
    "# for i in range(10000):\n",
    "#     print(i)\n",
    "#     imgs = []\n",
    "#     for n, cppn in enumerate(archive):\n",
    "#         img = cppn.generate_image((64, 64))\n",
    "#         for _ in range(4):\n",
    "#             imgs.append(augment_trans(img))\n",
    "            \n",
    "#     img_augments = torch.cat(imgs, dim=0)\n",
    "#     image_features = model.encode_image(img_augments)\n",
    "\n",
    "#     loss = 0\n",
    "#     for n in range(NUM_IMAGES):\n",
    "#         loss -= torch.cosine_similarity(image_features[4*n:4*(n+1)], text_features[n]).mean()\n",
    "\n",
    "#     optim.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optim.step()\n",
    "    \n",
    "#     if i % 100 == 0:\n",
    "#         display_archive(archive, nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_trans = transforms.Compose([\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=1, fill=1),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7,0.9)),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Initialize Archive\n",
    "archive = []\n",
    "scores = []\n",
    "updated = np.zeros(NUM_IMAGES)\n",
    "for i in range(NUM_IMAGES):\n",
    "    cppn = ImageCPPN(n_hidden=20, n_layers=8, n_channels=3, activation=torch.relu, give_radius=True).to(device)\n",
    "    archive.append(cppn)\n",
    "    scores.append(0)\n",
    "\n",
    "    \n",
    "for i in range(10000):\n",
    "    print(i)\n",
    "    if i % 10 == 0:\n",
    "        display_archive(archive, nouns, updated)\n",
    "        updated = np.zeros(NUM_IMAGES)\n",
    "    # Optimization iteration\n",
    "    copies = []\n",
    "    for cppn in archive:\n",
    "        state_dict = cppn.state_dict()\n",
    "        new_cppn = ImageCPPN(n_hidden=20, n_layers=8, n_channels=3, activation=torch.relu, give_radius=True).to(device)\n",
    "        new_cppn.load_state_dict(state_dict)\n",
    "        copies.append(new_cppn)\n",
    "    params = []\n",
    "    for cppn in copies:\n",
    "        params += list(cppn.parameters())\n",
    "    optim = torch.optim.Adam(params, lr=1e-3)\n",
    "    \n",
    "    # Assign targets randomly\n",
    "    targets = np.random.randint(NUM_IMAGES, size=(NUM_IMAGES))\n",
    "    for n in range(NUM_IMAGES):\n",
    "        if np.random.uniform() < 1:\n",
    "            targets[n] = n\n",
    "#     print(targets)\n",
    "    \n",
    "    # Run optimization for 20 iterations\n",
    "    for c in range(20):\n",
    "        imgs = []\n",
    "        for n, cppn in enumerate(copies):\n",
    "            img = cppn.generate_image((64, 64))\n",
    "            for _ in range(4):\n",
    "                imgs.append(augment_trans(img))\n",
    "\n",
    "        img_augments = torch.cat(imgs, dim=0)\n",
    "        image_features = model.encode_image(img_augments)\n",
    "\n",
    "        loss = 0\n",
    "        for n in range(NUM_IMAGES):\n",
    "            loss -= torch.cosine_similarity(image_features[4*n:4*(n+1)], text_features[n]).mean()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    for n in range(NUM_IMAGES):\n",
    "        archive[n].load_state_dict(copies[n].state_dict())\n",
    "    \n",
    "#     Check if image displaces any others\n",
    "    with torch.no_grad():\n",
    "        imgs = torch.stack([cppn.generate_image((64, 64))[0] for cppn in copies])\n",
    "        imgs = resize_trans(imgs)\n",
    "        imgs_features = model.encode_image(imgs) # 49 x 128\n",
    "        \n",
    "        for n in range(NUM_IMAGES):\n",
    "            clip_scores = torch.cosine_similarity(imgs_features[n], text_features, dim=-1)\n",
    "            for m in range(NUM_IMAGES):\n",
    "                if clip_scores[m] > scores[m]:\n",
    "                    updated[m] = True\n",
    "                    scores[m] = clip_scores[m].item()\n",
    "                    archive[m].load_state_dict(copies[n].state_dict())\n",
    "    del copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_archive(archive, nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frog",
   "language": "python",
   "name": "frog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
